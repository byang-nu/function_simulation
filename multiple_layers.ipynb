{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Network configuration variables\n",
    "input_size = 1\n",
    "num_gaussians = 50\n",
    "num_sigmoids = 0\n",
    "hidden_sizes = [50, 50]  # 2 hidden layers\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, num_gaussians, num_sigmoids, hidden_sizes, output_size, activation=\"relu\"):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.num_sigmoids = num_sigmoids\n",
    "        self.means = nn.Parameter(torch.randn(num_gaussians)) # means for gaussians tensor\n",
    "        self.origins = nn.Parameter(torch.randn(num_sigmoids)) # origins for sigmoids tensor\n",
    "        self.log_sigmas = nn.Parameter(torch.randn(num_gaussians)/4) # to keep initial sigmas reasonable\n",
    "        self.scales = nn.Parameter(torch.randn(num_sigmoids)/4) # to keep initial scales reasonable\n",
    "\n",
    "        # --- build an arbitrary stack of Linear layers ----\n",
    "        input_dim = num_gaussians + num_sigmoids\n",
    "        sizes = [input_dim] + list(hidden_sizes)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)]\n",
    "        )\n",
    "        self.out = nn.Linear(sizes[-1], output_size)\n",
    "\n",
    "        # activation    \n",
    "        acts = {\"relu\": nn.ReLU(), \"tanh\": nn.Tanh(), \"gelu\": nn.GELU()}\n",
    "            # ReLu (Rectified Linear Unit): f(x) = max(0, x) || ++ simple, efficient, works well for most networks || -- can die during training if too many neurons output 0\n",
    "            # Tanh (Hyperbolic Tangent): f(x) = tanh(x) || outputs between -1 and 1, zero-centered || ++ good for smooth function approximation || -- can suffer from vanishing gradient problem\n",
    "            # GELU (Gaussian Error Linear Unit): f(x) = x * Φ(x) || ++ smooth || -- computationally expansive\n",
    "        self.act = acts[activation]\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # init all hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            nn.init.normal_(layer.weight, mean=0, std=1)\n",
    "            nn.init.normal_(layer.bias,   mean=0, std=1)\n",
    "            layer.weight.data.clamp_(-1, 1)\n",
    "\n",
    "        # init output layer\n",
    "        nn.init.normal_(self.out.weight, mean=0, std=1)\n",
    "        nn.init.normal_(self.out.bias,   mean=0, std=1)\n",
    "        self.out.weight.data.clamp_(-1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1)\n",
    "\n",
    "        # ---- Gaussian features ----\n",
    "        if self.num_gaussians > 0:\n",
    "            mu = self.means.view(1, -1)\n",
    "            sigma = torch.exp(self.log_sigmas).view(1, -1)\n",
    "            x_expanded_gaus = x.expand(-1, self.num_gaussians)\n",
    "            gaussians = torch.exp(-0.5 * ((x_expanded_gaus - mu) / sigma) ** 2)\n",
    "        else:\n",
    "            gaussians = torch.empty(x.size(0), 0, device=x.device)\n",
    "\n",
    "        # ---- Sigmoid features ----\n",
    "        if self.num_sigmoids > 0:\n",
    "            origins = self.origins.view(1, -1)\n",
    "            scales  = torch.exp(self.scales).view(1, -1)\n",
    "            x_expanded_sigm = x.expand(-1, self.num_sigmoids)\n",
    "            sigmoids = 1.0 / (1.0 + torch.exp(-((x_expanded_sigm - origins) / scales)))\n",
    "        else:\n",
    "            sigmoids = torch.empty(x.size(0), 0, device=x.device)\n",
    "\n",
    "        h = torch.cat([gaussians, sigmoids], dim=1) # initialize the internal state of the network\n",
    "\n",
    "        # ---- arbitrary-depth MLP ----\n",
    "        for layer in self.hidden_layers:\n",
    "            h = self.act(layer(h)) # pass h through each hidden layer with activation (so non-linear)\n",
    "\n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bad5fd",
   "metadata": {},
   "source": [
    "Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe623a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = SimpleFNN(\n",
    "    num_gaussians=1,\n",
    "    num_sigmoids=0,\n",
    "    hidden_sizes=[32, 32],   \n",
    "    output_size=1,\n",
    "    activation=\"relu\",       # \"relu\" | \"tanh\" | \"gelu\"\n",
    ")\n",
    "\n",
    "x_eval = torch.linspace(-1, 1, 100).unsqueeze(1) # shape (100, 1)  [unsqueeze to add feature dimension]\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=1e-3) # Adam optimizer at learning rate 0.001\n",
    "criterion = nn.MSELoss() # Mean Squared Error loss  [how far your model’s predictions are from the target values]\n",
    "\n",
    "def l1_of_all_weights(model):\n",
    "    l1 = 0.0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            l1 = l1 + m.weight.abs().sum()\n",
    "    return l1\n",
    "\n",
    "previous_loss = 100\n",
    "for epoch in range(1000000):\n",
    "    optimizer.zero_grad() # zero the parameter gradients\n",
    "    y_pred = neural_net(x_eval)\n",
    "    y_true = torch.exp(x_eval) # target function: y = e^x\n",
    "\n",
    "    l1_norm = l1_of_all_weights(neural_net) # computes the sum of absolute values of all weights in all hidden layers\n",
    "    loss = criterion(y_pred, y_true)# + torch.tensor(0.002)*l1_norm\n",
    "    loss.backward() # backpropagation  [looks at how the loss changes nd calculates how each weight contributed to that loss]\n",
    "    optimizer.step() # update weights based on gradients calculated during backpropagation\n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item():.6f}\")\n",
    "        print(\"Amplitudes:\", neural_net.out.weight.data)\n",
    "        current_loss = loss\n",
    "        previous_loss = loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
